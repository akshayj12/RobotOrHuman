{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "sc = SparkContext( 'local', 'pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to access bucket\n",
    "def s3_access():\n",
    "    from boto.s3.connection import S3Connection\n",
    "    AWS_KEY = 'AKIAJCNESMIANLI6UXOQ'\n",
    "    AWS_SECRET = '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'\n",
    "    aws_connection = S3Connection(AWS_KEY, AWS_SECRET)\n",
    "    bucket = aws_connection.get_bucket('ml_project', validate=False)\n",
    "    bucket_list=bucket.list()\n",
    "    key=bucket.get_key('testlogs.txt')#'user_url_logs.txt')#testlogs.txt\n",
    "    v = '/Users/AkshayJain/Downloads/' + key.name\n",
    "    key.get_contents_to_filename(v) \n",
    "    return v\n",
    "\n",
    "#calling the access function and saving location of local file on variable\n",
    "input_file = s3_access()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global variables:\n",
    "categories = {}#To hold initial values for all found categories\n",
    "user_values =[]#To hold unique user ids along with corresponding list of categories for each\n",
    "#####################################################\n",
    "#Getting the data from local\n",
    "data = sc.textFile(input_file)#'/Users/patrickquerido/Downloads/exampledata2.txt')\n",
    "\n",
    "#This function groups all entries by key.\n",
    "#It will return a list of unique keys(users) along with corresponding list of categories.\n",
    "def group_By_Key(data):\n",
    "    return data.map(lambda line: line.split(',')).map(lambda x: (x[0],x[1])).groupByKey().mapValues(lambda x: list(x)).collect()\n",
    "\n",
    "#This function groups all entries by value\n",
    "#It will return a list with only unique values(distinct categories)\n",
    "def group_By_Value(data):\n",
    "    return data.map(lambda line: line.split(',')).map(lambda x : x[1]).distinct().collect()\n",
    "\n",
    "#This function will return a list of distinct users(no duplicates)\n",
    "def distinct_users(data):\n",
    "    return data.map(lambda line: line.split(',')).map(lambda x : x[0]).distinct().collect()\n",
    "\n",
    "##########################EXECUTE######################################\n",
    "\n",
    "#Function to assign values and keys to initial global variable dictionary\n",
    "#This is a function to generate random values(according to number of categories)\n",
    "#It also assigns values(categories) with corresponding random numbers\n",
    "#Global dictionary variable was created for this\n",
    "def distinct_categoriesDict():\n",
    "    category_helper = {}#helper dictionary to hold values to be assigned to global category dictionary variable\n",
    "    global categories#calling global categories dictionary\n",
    "    num_categories = len(group_By_Value(data))#counting number of unique categories\n",
    "    randon_gen = np.random.random(size=(num_categories,1))#generating rand # according to number of categories\n",
    "    catgrs = group_By_Value(data)#variable to hold all distinct categories\n",
    "    for rgen,cat in zip(randon_gen,catgrs):#iterating over and assigning\n",
    "        category_helper[cat] = rgen\n",
    "    categories = category_helper#assigning values from helper variable to global variable categories\n",
    "\n",
    "    \n",
    "#Function to assign list of categories to initial global variable of all user category list    \n",
    "def user_category_list():\n",
    "    user_values_helper = []#Helper list to hold list of categories per unique user\n",
    "    global user_values\n",
    "    user_categories = group_By_Key(data)#Variable to hold all unique users along with all list of categories\n",
    "    for lines in user_categories:\n",
    "        user_values_helper.append(lines[1])\n",
    "    user_values = user_values_helper#Assigning list to global variable\n",
    "    \n",
    "    \n",
    "#Function that takes the last list(list of all categories list per user)\n",
    "#User id actuall is not in the data\n",
    "#Returns same list but with numbers\n",
    "def user_list_value(user_list):\n",
    "    user_dic = {}\n",
    "    output_list = []\n",
    "    for key, value in categories.iteritems():\n",
    "        user_dic[key] = value\n",
    "    #SORT RIGHT HERE\n",
    "    #user_doc = sorted(user_dic.Keys())\n",
    "    for elems in user_list:\n",
    "        for key, value in user_dic.iteritems():\n",
    "            if elems == key:# == key:\n",
    "                user_dic[key]= value * value#'#value = value * value\n",
    "    for key, value in user_dic.iteritems():\n",
    "        for key2,value2 in categories.iteritems():\n",
    "            if value == value2:\n",
    "                user_dic[key]= [0.00000000]\n",
    "    for key,value in user_dic.iteritems():\n",
    "        output_list.append(value)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############EXECUTION########################\n",
    "  \n",
    "distinct_categoriesDict()   \n",
    "user_category_list() \n",
    "final_array = []#Final array to hold the final output\n",
    "#Function to output final: \n",
    "def final():\n",
    "    for items in user_values:\n",
    "        #print user_list_value(items)\n",
    "        a = user_list_value(items)\n",
    "        final_array.append(a)\n",
    "      \n",
    "    \n",
    "final() \n",
    "#print final_array\n",
    "#print np.array(final_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kmeans clustering algorithm\n",
    "# data = set of data points\n",
    "# k = number of clusters\n",
    "# c = initial list of centroids (if provided)\n",
    "#\n",
    "def kmeans(data, k, c):\n",
    "    centroids = []\n",
    "\n",
    "    centroids = randomize_centroids(data, centroids, k)  \n",
    "\n",
    "    old_centroids = [[] for i in range(k)] \n",
    "\n",
    "    iterations = 0\n",
    "    while not (has_converged(centroids, old_centroids, iterations)):\n",
    "        iterations += 1\n",
    "\n",
    "        clusters = [[] for i in range(k)]\n",
    "\n",
    "        # assign data points to clusters\n",
    "        clusters = euclidean_dist(data, centroids, clusters)\n",
    "\n",
    "        # recalculate centroids\n",
    "        index = 0\n",
    "        for cluster in clusters:\n",
    "            old_centroids[index] = centroids[index]\n",
    "            centroids[index] = np.mean(cluster, axis=0).tolist()\n",
    "            index += 1\n",
    "\n",
    "\n",
    "    print(\"The total number of data instances is: \" + str(len(data)))\n",
    "    print(\"The total number of iterations necessary is: \" + str(iterations))\n",
    "    print(\"The means of each cluster are: \" + str(centroids))\n",
    "    print(\"The clusters are as follows:\")\n",
    "    for cluster in clusters:\n",
    "        print(\"Cluster with a size of \" + str(len(cluster)) + \" starts here:\")\n",
    "        print(np.array(cluster).tolist())\n",
    "        print(\"Cluster ends here.\")\n",
    "\n",
    "    return\n",
    "\n",
    "# Calculates euclidean distance between\n",
    "# a data point and all the available cluster\n",
    "# centroids.      \n",
    "def euclidean_dist(data, centroids, clusters):\n",
    "    for instance in data:  \n",
    "        # Find which centroid is the closest\n",
    "        # to the given data point.\n",
    "        mu_index = min([(i[0], np.linalg.norm(instance-centroids[i[0]])) \\\n",
    "                            for i in enumerate(centroids)], key=lambda t:t[1])[0]\n",
    "        try:\n",
    "            clusters[mu_index].append(instance)\n",
    "        except KeyError:\n",
    "            clusters[mu_index] = [instance]\n",
    "\n",
    "    # If any cluster is empty then assign one point\n",
    "    # from data set randomly so as to not have empty\n",
    "    # clusters and 0 means.        \n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            cluster.append(data[np.random.randint(0, len(data), size=1)].flatten().tolist())\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# randomize initial centroids\n",
    "def randomize_centroids(data, centroids, k):\n",
    "    for cluster in range(0, k):\n",
    "        centroids.append(data[np.random.randint(0, len(data), size=1)].flatten().tolist())\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# check if clusters have converged    \n",
    "def has_converged(centroids, old_centroids, iterations):\n",
    "    MAX_ITERATIONS = 1000\n",
    "    if iterations > MAX_ITERATIONS:\n",
    "        return True\n",
    "    return old_centroids == centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapoint = np.array(final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of data instances is: 4\n",
      "The total number of iterations necessary is: 4\n",
      "The means of each cluster are: [[[0.002009997988154741], [0.1636467838879947], [0.2458369400290434], [0.18062113949525463]], [0.0, 0.1636467838879947, 0.0, 0.0]]\n",
      "The clusters are as follows:\n",
      "Cluster with a size of 4 starts here:\n",
      "[[[0.008039991952618964], [0.1636467838879947], [0.0], [0.7224845579810185]], [[0.0], [0.1636467838879947], [0.9833477601161736], [0.0]], [[0.0], [0.1636467838879947], [0.0], [0.0]], [[0.0], [0.1636467838879947], [0.0], [0.0]]]\n",
      "Cluster ends here.\n",
      "Cluster with a size of 1 starts here:\n",
      "[[0.0, 0.1636467838879947, 0.0, 0.0]]\n",
      "Cluster ends here.\n"
     ]
    }
   ],
   "source": [
    "kmeans(datapoint,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"'8fab7374-ca5a-4531-a757-bfc461b30169',article\", u\"'2f2b503a-743e-41b9-965e-1175b7975a00',article\", u\"'d43a54c1-a6f8-4e1a-a10f-206fac61049d',article\", u\"'bba7a2ff-2477-4e50-8acc-7c23702348ac',article\", u\"'d43a54c1-a6f8-4e1a-a10f-206fac61049d',news\", u\"'d43a54c1-a6f8-4e1a-a10f-206fac61049d',TV\", u\"'8fab7374-ca5a-4531-a757-bfc461b30169',Q\"]\n"
     ]
    }
   ],
   "source": [
    "print data.map(lambda line:line).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 0.00803999]), array([ 0.16364678]), [0.0], array([ 0.72248456])], [[0.0], array([ 0.16364678]), array([ 0.98334776]), [0.0]], [[0.0], array([ 0.16364678]), [0.0], [0.0]], [[0.0], array([ 0.16364678]), [0.0], [0.0]]]\n"
     ]
    }
   ],
   "source": [
    "print final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'article', u'news', u'TV'], [u'article', u'Q'], [u'article'], [u'article']]\n"
     ]
    }
   ],
   "source": [
    "print user_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
